{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from gan import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GAN for novel sample generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Batch 0/2210 D Loss: 1.2940 G Loss: 11.1314\n",
      "Epoch [1/100] Batch 50/2210 D Loss: 0.6072 G Loss: 12.3911\n",
      "Epoch [1/100] Batch 100/2210 D Loss: 5.3392 G Loss: 15.1707\n",
      "Epoch [1/100] Batch 150/2210 D Loss: 0.2516 G Loss: 6.9509\n",
      "Epoch [1/100] Batch 200/2210 D Loss: 0.0293 G Loss: 12.7179\n",
      "Epoch [1/100] Batch 250/2210 D Loss: 0.0049 G Loss: 10.1421\n",
      "Epoch [1/100] Batch 300/2210 D Loss: 0.0889 G Loss: 4.3129\n",
      "Epoch [1/100] Batch 350/2210 D Loss: 0.0040 G Loss: 7.1898\n",
      "Epoch [1/100] Batch 400/2210 D Loss: 0.1445 G Loss: 5.5973\n",
      "Epoch [1/100] Batch 450/2210 D Loss: 0.0121 G Loss: 6.7306\n",
      "Epoch [1/100] Batch 500/2210 D Loss: 0.0269 G Loss: 5.7324\n",
      "Epoch [1/100] Batch 550/2210 D Loss: 0.0902 G Loss: 8.2837\n",
      "Epoch [1/100] Batch 600/2210 D Loss: 0.0400 G Loss: 8.4774\n",
      "Epoch [1/100] Batch 650/2210 D Loss: 0.2640 G Loss: 5.5260\n",
      "Epoch [1/100] Batch 700/2210 D Loss: 0.0592 G Loss: 3.6554\n",
      "Epoch [1/100] Batch 750/2210 D Loss: 0.1384 G Loss: 4.3626\n",
      "Epoch [1/100] Batch 800/2210 D Loss: 0.0094 G Loss: 7.6987\n",
      "Epoch [1/100] Batch 850/2210 D Loss: 0.0119 G Loss: 6.0709\n",
      "Epoch [1/100] Batch 900/2210 D Loss: 0.0521 G Loss: 7.0560\n",
      "Epoch [1/100] Batch 950/2210 D Loss: 1.3401 G Loss: 0.0613\n",
      "Epoch [1/100] Batch 1000/2210 D Loss: 0.1748 G Loss: 3.4835\n",
      "Epoch [1/100] Batch 1050/2210 D Loss: 0.0012 G Loss: 8.8625\n",
      "Epoch [1/100] Batch 1100/2210 D Loss: 0.0061 G Loss: 8.5127\n",
      "Epoch [1/100] Batch 1150/2210 D Loss: 0.0027 G Loss: 6.1201\n",
      "Epoch [1/100] Batch 1200/2210 D Loss: 0.0026 G Loss: 7.9849\n",
      "Epoch [1/100] Batch 1250/2210 D Loss: 0.0352 G Loss: 11.2169\n",
      "Epoch [1/100] Batch 1300/2210 D Loss: 0.0118 G Loss: 5.1673\n",
      "Epoch [1/100] Batch 1350/2210 D Loss: 0.0391 G Loss: 4.7821\n",
      "Epoch [1/100] Batch 1400/2210 D Loss: 0.0270 G Loss: 4.2479\n",
      "Epoch [1/100] Batch 1450/2210 D Loss: 0.0112 G Loss: 4.6376\n",
      "Epoch [1/100] Batch 1500/2210 D Loss: 0.0314 G Loss: 4.4517\n",
      "Epoch [1/100] Batch 1550/2210 D Loss: 0.1956 G Loss: 3.4368\n",
      "Epoch [1/100] Batch 1600/2210 D Loss: 0.0230 G Loss: 4.3545\n",
      "Epoch [1/100] Batch 1650/2210 D Loss: 0.0009 G Loss: 9.8075\n",
      "Epoch [1/100] Batch 1700/2210 D Loss: 0.0273 G Loss: 5.0855\n",
      "Epoch [1/100] Batch 1750/2210 D Loss: 0.0381 G Loss: 4.7638\n",
      "Epoch [1/100] Batch 1800/2210 D Loss: 0.0024 G Loss: 6.2036\n",
      "Epoch [1/100] Batch 1850/2210 D Loss: 0.0093 G Loss: 5.2164\n",
      "Epoch [1/100] Batch 1900/2210 D Loss: 0.0092 G Loss: 4.9468\n",
      "Epoch [1/100] Batch 1950/2210 D Loss: 0.3790 G Loss: 4.9725\n",
      "Epoch [1/100] Batch 2000/2210 D Loss: 0.0079 G Loss: 9.6571\n",
      "Epoch [1/100] Batch 2050/2210 D Loss: 0.0130 G Loss: 8.5623\n",
      "Epoch [1/100] Batch 2100/2210 D Loss: 0.0175 G Loss: 4.4539\n",
      "Epoch [1/100] Batch 2150/2210 D Loss: 0.0027 G Loss: 6.7912\n",
      "Epoch [1/100] Batch 2200/2210 D Loss: 0.0416 G Loss: 3.8938\n",
      "Epoch [1/100] D Loss: 0.0860 G Loss: 4.3841\n",
      "Epoch [2/100] Batch 0/2210 D Loss: 0.0601 G Loss: 4.3054\n",
      "Epoch [2/100] Batch 50/2210 D Loss: 0.0161 G Loss: 4.4910\n",
      "Epoch [2/100] Batch 100/2210 D Loss: 0.6850 G Loss: 7.7668\n",
      "Epoch [2/100] Batch 150/2210 D Loss: 0.0176 G Loss: 4.5716\n",
      "Epoch [2/100] Batch 200/2210 D Loss: 0.2017 G Loss: 3.0419\n",
      "Epoch [2/100] Batch 250/2210 D Loss: 0.0051 G Loss: 5.4500\n",
      "Epoch [2/100] Batch 300/2210 D Loss: 0.0633 G Loss: 4.5329\n",
      "Epoch [2/100] Batch 350/2210 D Loss: 0.0183 G Loss: 4.5362\n",
      "Epoch [2/100] Batch 400/2210 D Loss: 0.1260 G Loss: 2.9612\n",
      "Epoch [2/100] Batch 450/2210 D Loss: 0.0171 G Loss: 6.6310\n",
      "Epoch [2/100] Batch 500/2210 D Loss: 0.0347 G Loss: 4.5467\n",
      "Epoch [2/100] Batch 550/2210 D Loss: 0.0120 G Loss: 5.6057\n",
      "Epoch [2/100] Batch 600/2210 D Loss: 0.0226 G Loss: 4.1003\n",
      "Epoch [2/100] Batch 650/2210 D Loss: 0.0048 G Loss: 8.1076\n",
      "Epoch [2/100] Batch 700/2210 D Loss: 0.3319 G Loss: 2.7916\n",
      "Epoch [2/100] Batch 750/2210 D Loss: 0.0055 G Loss: 5.9037\n",
      "Epoch [2/100] Batch 800/2210 D Loss: 0.7688 G Loss: 3.8549\n",
      "Epoch [2/100] Batch 850/2210 D Loss: 0.0021 G Loss: 6.7363\n",
      "Epoch [2/100] Batch 900/2210 D Loss: 1.1581 G Loss: 4.2245\n",
      "Epoch [2/100] Batch 950/2210 D Loss: 0.0097 G Loss: 4.6707\n",
      "Epoch [2/100] Batch 1000/2210 D Loss: 0.0021 G Loss: 6.7640\n",
      "Epoch [2/100] Batch 1050/2210 D Loss: 0.0323 G Loss: 4.2199\n",
      "Epoch [2/100] Batch 1100/2210 D Loss: 0.0109 G Loss: 5.0827\n",
      "Epoch [2/100] Batch 1150/2210 D Loss: 0.0204 G Loss: 4.0204\n",
      "Epoch [2/100] Batch 1200/2210 D Loss: 0.0105 G Loss: 4.6982\n",
      "Epoch [2/100] Batch 1250/2210 D Loss: 0.0025 G Loss: 6.6980\n",
      "Epoch [2/100] Batch 1300/2210 D Loss: 3.1113 G Loss: 0.7159\n",
      "Epoch [2/100] Batch 1350/2210 D Loss: 0.0636 G Loss: 3.1364\n",
      "Epoch [2/100] Batch 1400/2210 D Loss: 0.0250 G Loss: 3.8888\n",
      "Epoch [2/100] Batch 1450/2210 D Loss: 0.0125 G Loss: 4.5872\n",
      "Epoch [2/100] Batch 1500/2210 D Loss: 0.0170 G Loss: 4.2395\n",
      "Epoch [2/100] Batch 1550/2210 D Loss: 0.0627 G Loss: 3.5065\n",
      "Epoch [2/100] Batch 1600/2210 D Loss: 0.0136 G Loss: 4.5284\n",
      "Epoch [2/100] Batch 1650/2210 D Loss: 0.0086 G Loss: 5.0372\n",
      "Epoch [2/100] Batch 1700/2210 D Loss: 0.1631 G Loss: 2.5497\n",
      "Epoch [2/100] Batch 1750/2210 D Loss: 0.2657 G Loss: 2.1365\n",
      "Epoch [2/100] Batch 1800/2210 D Loss: 0.0160 G Loss: 4.2722\n",
      "Epoch [2/100] Batch 1850/2210 D Loss: 0.0301 G Loss: 4.2993\n",
      "Epoch [2/100] Batch 1900/2210 D Loss: 0.0499 G Loss: 3.7222\n",
      "Epoch [2/100] Batch 1950/2210 D Loss: 0.0051 G Loss: 4.2435\n",
      "Epoch [2/100] Batch 2000/2210 D Loss: 0.0245 G Loss: 4.5054\n",
      "Epoch [2/100] Batch 2050/2210 D Loss: 0.4723 G Loss: 2.9431\n",
      "Epoch [2/100] Batch 2100/2210 D Loss: 0.0044 G Loss: 6.5277\n",
      "Epoch [2/100] Batch 2150/2210 D Loss: 0.0079 G Loss: 6.7671\n",
      "Epoch [2/100] Batch 2200/2210 D Loss: 0.0409 G Loss: 4.0033\n",
      "Epoch [2/100] D Loss: 0.0302 G Loss: 3.9001\n",
      "Epoch [3/100] Batch 0/2210 D Loss: 0.0329 G Loss: 3.7920\n",
      "Epoch [3/100] Batch 50/2210 D Loss: 0.0002 G Loss: 11.0779\n",
      "Epoch [3/100] Batch 100/2210 D Loss: 0.0061 G Loss: 6.4719\n",
      "Epoch [3/100] Batch 150/2210 D Loss: 0.1812 G Loss: 4.1523\n",
      "Epoch [3/100] Batch 200/2210 D Loss: 0.0322 G Loss: 3.6750\n",
      "Epoch [3/100] Batch 250/2210 D Loss: 0.0685 G Loss: 4.4897\n",
      "Epoch [3/100] Batch 300/2210 D Loss: 0.0007 G Loss: 7.9937\n",
      "Epoch [3/100] Batch 350/2210 D Loss: 0.1503 G Loss: 3.2111\n",
      "Epoch [3/100] Batch 400/2210 D Loss: 0.1124 G Loss: 2.6482\n",
      "Epoch [3/100] Batch 450/2210 D Loss: 0.0118 G Loss: 4.9101\n",
      "Epoch [3/100] Batch 500/2210 D Loss: 0.4830 G Loss: 6.0375\n",
      "Epoch [3/100] Batch 550/2210 D Loss: 0.0155 G Loss: 4.3390\n",
      "Epoch [3/100] Batch 600/2210 D Loss: 0.0002 G Loss: 10.4856\n",
      "Epoch [3/100] Batch 650/2210 D Loss: 0.0091 G Loss: 5.0843\n",
      "Epoch [3/100] Batch 700/2210 D Loss: 0.0039 G Loss: 5.5563\n",
      "Epoch [3/100] Batch 750/2210 D Loss: 0.0263 G Loss: 4.3835\n",
      "Epoch [3/100] Batch 800/2210 D Loss: 0.0253 G Loss: 4.2300\n",
      "Epoch [3/100] Batch 850/2210 D Loss: 0.0646 G Loss: 4.7769\n",
      "Epoch [3/100] Batch 900/2210 D Loss: 0.0295 G Loss: 3.1987\n",
      "Epoch [3/100] Batch 950/2210 D Loss: 0.1845 G Loss: 3.9271\n",
      "Epoch [3/100] Batch 1000/2210 D Loss: 0.0396 G Loss: 3.8423\n",
      "Epoch [3/100] Batch 1050/2210 D Loss: 0.0742 G Loss: 3.8488\n",
      "Epoch [3/100] Batch 1100/2210 D Loss: 0.0261 G Loss: 3.8752\n",
      "Epoch [3/100] Batch 1150/2210 D Loss: 0.2626 G Loss: 2.6851\n",
      "Epoch [3/100] Batch 1200/2210 D Loss: 0.0423 G Loss: 3.5325\n",
      "Epoch [3/100] Batch 1250/2210 D Loss: 0.0118 G Loss: 5.1913\n",
      "Epoch [3/100] Batch 1300/2210 D Loss: 0.0402 G Loss: 3.9050\n",
      "Epoch [3/100] Batch 1350/2210 D Loss: 0.0736 G Loss: 4.5860\n",
      "Epoch [3/100] Batch 1400/2210 D Loss: 0.0054 G Loss: 6.9017\n",
      "Epoch [3/100] Batch 1450/2210 D Loss: 0.1548 G Loss: 4.5281\n",
      "Epoch [3/100] Batch 1500/2210 D Loss: 0.1078 G Loss: 4.7377\n",
      "Epoch [3/100] Batch 1550/2210 D Loss: 0.0341 G Loss: 4.4074\n",
      "Epoch [3/100] Batch 1600/2210 D Loss: 0.0022 G Loss: 8.7389\n",
      "Epoch [3/100] Batch 1650/2210 D Loss: 0.0240 G Loss: 4.3745\n",
      "Epoch [3/100] Batch 1700/2210 D Loss: 0.0376 G Loss: 4.0269\n",
      "Epoch [3/100] Batch 1750/2210 D Loss: 0.0489 G Loss: 3.7832\n",
      "Epoch [3/100] Batch 1800/2210 D Loss: 0.0466 G Loss: 3.9135\n",
      "Epoch [3/100] Batch 1850/2210 D Loss: 0.0177 G Loss: 4.2120\n",
      "Epoch [3/100] Batch 1900/2210 D Loss: 3.5600 G Loss: 1.0465\n",
      "Epoch [3/100] Batch 1950/2210 D Loss: 0.0046 G Loss: 5.4790\n",
      "Epoch [3/100] Batch 2000/2210 D Loss: 0.0053 G Loss: 5.5266\n",
      "Epoch [3/100] Batch 2050/2210 D Loss: 0.0054 G Loss: 6.1498\n",
      "Epoch [3/100] Batch 2100/2210 D Loss: 0.0767 G Loss: 3.2331\n",
      "Epoch [3/100] Batch 2150/2210 D Loss: 0.0411 G Loss: 3.6472\n",
      "Epoch [3/100] Batch 2200/2210 D Loss: 0.0400 G Loss: 3.5518\n",
      "Epoch [3/100] D Loss: 0.1865 G Loss: 1.7656\n",
      "Epoch [4/100] Batch 0/2210 D Loss: 1.8579 G Loss: 1.7259\n",
      "Epoch [4/100] Batch 50/2210 D Loss: 6.0850 G Loss: 2.8759\n",
      "Epoch [4/100] Batch 100/2210 D Loss: 0.0561 G Loss: 3.2795\n",
      "Epoch [4/100] Batch 150/2210 D Loss: 0.0369 G Loss: 4.2375\n",
      "Epoch [4/100] Batch 200/2210 D Loss: 0.0774 G Loss: 3.7739\n",
      "Epoch [4/100] Batch 250/2210 D Loss: 0.0322 G Loss: 3.9442\n",
      "Epoch [4/100] Batch 300/2210 D Loss: 0.0064 G Loss: 5.4735\n",
      "Epoch [4/100] Batch 350/2210 D Loss: 0.0165 G Loss: 4.3404\n",
      "Epoch [4/100] Batch 400/2210 D Loss: 0.0085 G Loss: 5.5735\n",
      "Epoch [4/100] Batch 450/2210 D Loss: 0.0112 G Loss: 4.7188\n",
      "Epoch [4/100] Batch 500/2210 D Loss: 0.0075 G Loss: 5.0163\n",
      "Epoch [4/100] Batch 550/2210 D Loss: 0.0133 G Loss: 4.5779\n",
      "Epoch [4/100] Batch 600/2210 D Loss: 0.0001 G Loss: 9.1432\n",
      "Epoch [4/100] Batch 650/2210 D Loss: 7.2109 G Loss: 0.6571\n",
      "Epoch [4/100] Batch 700/2210 D Loss: 0.1807 G Loss: 3.5107\n",
      "Epoch [4/100] Batch 750/2210 D Loss: 0.0162 G Loss: 4.4395\n",
      "Epoch [4/100] Batch 800/2210 D Loss: 0.0279 G Loss: 3.8361\n",
      "Epoch [4/100] Batch 850/2210 D Loss: 0.0001 G Loss: 9.4182\n",
      "Epoch [4/100] Batch 900/2210 D Loss: 0.0532 G Loss: 3.4743\n",
      "Epoch [4/100] Batch 950/2210 D Loss: 0.5248 G Loss: 2.7234\n",
      "Epoch [4/100] Batch 1000/2210 D Loss: 1.2840 G Loss: 2.7967\n",
      "Epoch [4/100] Batch 1050/2210 D Loss: 0.2862 G Loss: 1.9210\n",
      "Epoch [4/100] Batch 1100/2210 D Loss: 1.4245 G Loss: 3.3037\n",
      "Epoch [4/100] Batch 1150/2210 D Loss: 0.0071 G Loss: 5.0452\n",
      "Epoch [4/100] Batch 1200/2210 D Loss: 0.0484 G Loss: 3.6183\n",
      "Epoch [4/100] Batch 1250/2210 D Loss: 0.0097 G Loss: 5.6660\n",
      "Epoch [4/100] Batch 1300/2210 D Loss: 0.0251 G Loss: 4.0964\n",
      "Epoch [4/100] Batch 1350/2210 D Loss: 0.0016 G Loss: 6.8882\n",
      "Epoch [4/100] Batch 1400/2210 D Loss: 0.0149 G Loss: 4.3825\n",
      "Epoch [4/100] Batch 1450/2210 D Loss: 0.0009 G Loss: 7.1134\n",
      "Epoch [4/100] Batch 1500/2210 D Loss: 0.0014 G Loss: 6.9048\n",
      "Epoch [4/100] Batch 1550/2210 D Loss: 0.0569 G Loss: 3.3434\n",
      "Epoch [4/100] Batch 1600/2210 D Loss: 0.0144 G Loss: 4.5021\n",
      "Epoch [4/100] Batch 1650/2210 D Loss: 0.2343 G Loss: 3.9587\n",
      "Epoch [4/100] Batch 1700/2210 D Loss: 0.2577 G Loss: 3.3480\n",
      "Epoch [4/100] Batch 1750/2210 D Loss: 0.0122 G Loss: 5.1352\n",
      "Epoch [4/100] Batch 1800/2210 D Loss: 0.0141 G Loss: 4.7083\n",
      "Epoch [4/100] Batch 1850/2210 D Loss: 0.0316 G Loss: 3.7658\n",
      "Epoch [4/100] Batch 1900/2210 D Loss: 0.0127 G Loss: 4.5442\n",
      "Epoch [4/100] Batch 1950/2210 D Loss: 0.0173 G Loss: 4.1794\n",
      "Epoch [4/100] Batch 2000/2210 D Loss: 0.0084 G Loss: 4.8209\n",
      "Epoch [4/100] Batch 2050/2210 D Loss: 0.0034 G Loss: 5.6244\n",
      "Epoch [4/100] Batch 2100/2210 D Loss: 0.0286 G Loss: 4.0027\n",
      "Epoch [4/100] Batch 2150/2210 D Loss: 0.0590 G Loss: 3.3155\n",
      "Epoch [4/100] Batch 2200/2210 D Loss: 0.0743 G Loss: 4.3300\n",
      "Epoch [4/100] D Loss: 0.0542 G Loss: 1.8936\n",
      "Epoch [5/100] Batch 0/2210 D Loss: 0.3278 G Loss: 2.3528\n",
      "Epoch [5/100] Batch 50/2210 D Loss: 4.2936 G Loss: 0.9150\n",
      "Epoch [5/100] Batch 100/2210 D Loss: 0.0708 G Loss: 4.9034\n",
      "Epoch [5/100] Batch 150/2210 D Loss: 0.0076 G Loss: 5.0550\n",
      "Epoch [5/100] Batch 200/2210 D Loss: 0.0645 G Loss: 3.8372\n",
      "Epoch [5/100] Batch 250/2210 D Loss: 0.0117 G Loss: 4.5710\n",
      "Epoch [5/100] Batch 300/2210 D Loss: 0.0401 G Loss: 3.6385\n",
      "Epoch [5/100] Batch 350/2210 D Loss: 1.7349 G Loss: 0.3578\n",
      "Epoch [5/100] Batch 400/2210 D Loss: 0.0703 G Loss: 3.7278\n",
      "Epoch [5/100] Batch 450/2210 D Loss: 0.0653 G Loss: 3.4475\n",
      "Epoch [5/100] Batch 500/2210 D Loss: 0.2698 G Loss: 3.5089\n",
      "Epoch [5/100] Batch 550/2210 D Loss: 0.3624 G Loss: 3.8940\n",
      "Epoch [5/100] Batch 600/2210 D Loss: 0.0006 G Loss: 7.5445\n",
      "Epoch [5/100] Batch 650/2210 D Loss: 0.0672 G Loss: 3.3481\n",
      "Epoch [5/100] Batch 700/2210 D Loss: 0.0371 G Loss: 4.0022\n",
      "Epoch [5/100] Batch 750/2210 D Loss: 0.0476 G Loss: 3.4923\n",
      "Epoch [5/100] Batch 800/2210 D Loss: 0.0363 G Loss: 6.9992\n",
      "Epoch [5/100] Batch 850/2210 D Loss: 0.0317 G Loss: 3.7605\n",
      "Epoch [5/100] Batch 900/2210 D Loss: 0.0812 G Loss: 2.5268\n",
      "Epoch [5/100] Batch 950/2210 D Loss: 0.0275 G Loss: 5.1026\n",
      "Epoch [5/100] Batch 1000/2210 D Loss: 0.0013 G Loss: 6.7584\n",
      "Epoch [5/100] Batch 1050/2210 D Loss: 0.0554 G Loss: 4.2375\n",
      "Epoch [5/100] Batch 1100/2210 D Loss: 0.0397 G Loss: 3.7980\n",
      "Epoch [5/100] Batch 1150/2210 D Loss: 0.0111 G Loss: 4.6939\n",
      "Epoch [5/100] Batch 1200/2210 D Loss: 0.0147 G Loss: 5.3405\n",
      "Epoch [5/100] Batch 1250/2210 D Loss: 1.6607 G Loss: 4.2240\n",
      "Epoch [5/100] Batch 1300/2210 D Loss: 0.0007 G Loss: 7.9157\n",
      "Epoch [5/100] Batch 1350/2210 D Loss: 0.2899 G Loss: 2.9287\n",
      "Epoch [5/100] Batch 1400/2210 D Loss: 0.0311 G Loss: 4.9173\n",
      "Epoch [5/100] Batch 1450/2210 D Loss: 0.0005 G Loss: 7.8018\n",
      "Epoch [5/100] Batch 1500/2210 D Loss: 0.0099 G Loss: 4.9782\n",
      "Epoch [5/100] Batch 1550/2210 D Loss: 0.0271 G Loss: 3.8875\n",
      "Epoch [5/100] Batch 1600/2210 D Loss: 0.1595 G Loss: 3.8602\n",
      "Epoch [5/100] Batch 1650/2210 D Loss: 0.2016 G Loss: 3.8788\n",
      "Epoch [5/100] Batch 1700/2210 D Loss: 0.0666 G Loss: 3.6201\n",
      "Epoch [5/100] Batch 1750/2210 D Loss: 0.0019 G Loss: 6.4339\n",
      "Epoch [5/100] Batch 1800/2210 D Loss: 0.0408 G Loss: 3.8439\n",
      "Epoch [5/100] Batch 1850/2210 D Loss: 0.0320 G Loss: 3.7629\n",
      "Epoch [5/100] Batch 1900/2210 D Loss: 0.0028 G Loss: 6.0170\n",
      "Epoch [5/100] Batch 1950/2210 D Loss: 0.0016 G Loss: 6.3852\n",
      "Epoch [5/100] Batch 2000/2210 D Loss: 0.0242 G Loss: 4.6195\n",
      "Epoch [5/100] Batch 2050/2210 D Loss: 0.0325 G Loss: 3.7140\n",
      "Epoch [5/100] Batch 2100/2210 D Loss: 0.1663 G Loss: 3.1186\n",
      "Epoch [5/100] Batch 2150/2210 D Loss: 0.0467 G Loss: 3.2736\n",
      "Epoch [5/100] Batch 2200/2210 D Loss: 1.2581 G Loss: 2.2864\n",
      "Epoch [5/100] D Loss: 0.0382 G Loss: 3.6230\n",
      "Epoch [6/100] Batch 0/2210 D Loss: 0.0046 G Loss: 5.5473\n",
      "Epoch [6/100] Batch 50/2210 D Loss: 0.0083 G Loss: 4.8710\n",
      "Epoch [6/100] Batch 100/2210 D Loss: 0.0176 G Loss: 13.6806\n",
      "Epoch [6/100] Batch 150/2210 D Loss: 0.0006 G Loss: 8.6342\n",
      "Epoch [6/100] Batch 200/2210 D Loss: 0.0240 G Loss: 4.3207\n",
      "Epoch [6/100] Batch 250/2210 D Loss: 0.1054 G Loss: 2.7952\n",
      "Epoch [6/100] Batch 300/2210 D Loss: 0.0332 G Loss: 4.0332\n",
      "Epoch [6/100] Batch 350/2210 D Loss: 0.0129 G Loss: 4.3856\n",
      "Epoch [6/100] Batch 400/2210 D Loss: 4.1936 G Loss: 3.5488\n",
      "Epoch [6/100] Batch 450/2210 D Loss: 0.0040 G Loss: 5.6465\n",
      "Epoch [6/100] Batch 500/2210 D Loss: 1.2697 G Loss: 2.6795\n",
      "Epoch [6/100] Batch 550/2210 D Loss: 0.4278 G Loss: 2.6216\n",
      "Epoch [6/100] Batch 600/2210 D Loss: 0.2281 G Loss: 2.8199\n",
      "Epoch [6/100] Batch 650/2210 D Loss: 0.0058 G Loss: 5.2583\n",
      "Epoch [6/100] Batch 700/2210 D Loss: 0.0168 G Loss: 4.6210\n",
      "Epoch [6/100] Batch 750/2210 D Loss: 0.0460 G Loss: 2.3207\n",
      "Epoch [6/100] Batch 800/2210 D Loss: 0.0122 G Loss: 4.5793\n",
      "Epoch [6/100] Batch 850/2210 D Loss: 0.0477 G Loss: 3.5545\n",
      "Epoch [6/100] Batch 900/2210 D Loss: 11.5519 G Loss: 2.4775\n",
      "Epoch [6/100] Batch 950/2210 D Loss: 0.2314 G Loss: 3.7668\n",
      "Epoch [6/100] Batch 1000/2210 D Loss: 0.6372 G Loss: 4.1763\n",
      "Epoch [6/100] Batch 1050/2210 D Loss: 0.0175 G Loss: 4.4038\n",
      "Epoch [6/100] Batch 1100/2210 D Loss: 3.1728 G Loss: 6.6944\n",
      "Epoch [6/100] Batch 1150/2210 D Loss: 0.0263 G Loss: 4.9687\n",
      "Epoch [6/100] Batch 1200/2210 D Loss: 0.1352 G Loss: 2.5326\n",
      "Epoch [6/100] Batch 1250/2210 D Loss: 1.9162 G Loss: 3.5919\n",
      "Epoch [6/100] Batch 1300/2210 D Loss: 0.1470 G Loss: 1.5764\n",
      "Epoch [6/100] Batch 1350/2210 D Loss: 0.0021 G Loss: 6.6138\n",
      "Epoch [6/100] Batch 1400/2210 D Loss: 0.6380 G Loss: 2.4820\n",
      "Epoch [6/100] Batch 1450/2210 D Loss: 0.0463 G Loss: 3.5087\n",
      "Epoch [6/100] Batch 1500/2210 D Loss: 0.0232 G Loss: 3.9531\n",
      "Epoch [6/100] Batch 1550/2210 D Loss: 0.0093 G Loss: 4.7890\n",
      "Epoch [6/100] Batch 1600/2210 D Loss: 0.0788 G Loss: 3.1819\n",
      "Epoch [6/100] Batch 1650/2210 D Loss: 0.0582 G Loss: 3.4954\n",
      "Epoch [6/100] Batch 1700/2210 D Loss: 0.2775 G Loss: 2.2206\n",
      "Epoch [6/100] Batch 1750/2210 D Loss: 0.0999 G Loss: 3.6029\n",
      "Epoch [6/100] Batch 1800/2210 D Loss: 2.4522 G Loss: 1.9744\n",
      "Epoch [6/100] Batch 1850/2210 D Loss: 0.0568 G Loss: 3.3123\n",
      "Epoch [6/100] Batch 1900/2210 D Loss: 0.0127 G Loss: 4.7925\n",
      "Epoch [6/100] Batch 1950/2210 D Loss: 0.0304 G Loss: 3.7130\n",
      "Epoch [6/100] Batch 2000/2210 D Loss: 0.0017 G Loss: 6.4964\n",
      "Epoch [6/100] Batch 2050/2210 D Loss: 0.0098 G Loss: 5.6891\n",
      "Epoch [6/100] Batch 2100/2210 D Loss: 0.0050 G Loss: 5.4755\n",
      "Epoch [6/100] Batch 2150/2210 D Loss: 0.0025 G Loss: 6.0222\n",
      "Epoch [6/100] Batch 2200/2210 D Loss: 0.0772 G Loss: 3.5991\n",
      "Epoch [6/100] D Loss: 0.0013 G Loss: 9.5786\n",
      "Epoch [7/100] Batch 0/2210 D Loss: 0.0013 G Loss: 7.8443\n",
      "Epoch [7/100] Batch 50/2210 D Loss: 0.0115 G Loss: 4.8010\n",
      "Epoch [7/100] Batch 100/2210 D Loss: 0.0351 G Loss: 3.7385\n",
      "Epoch [7/100] Batch 150/2210 D Loss: 0.0135 G Loss: 4.4088\n",
      "Epoch [7/100] Batch 200/2210 D Loss: 0.0098 G Loss: 4.8589\n",
      "Epoch [7/100] Batch 250/2210 D Loss: 0.0131 G Loss: 4.5827\n",
      "Epoch [7/100] Batch 300/2210 D Loss: 1.6805 G Loss: 4.8311\n",
      "Epoch [7/100] Batch 350/2210 D Loss: 0.1304 G Loss: 3.2303\n",
      "Epoch [7/100] Batch 400/2210 D Loss: 0.1720 G Loss: 3.0346\n",
      "Epoch [7/100] Batch 450/2210 D Loss: 0.0010 G Loss: 7.0665\n",
      "Epoch [7/100] Batch 500/2210 D Loss: 0.0240 G Loss: 3.5027\n",
      "Epoch [7/100] Batch 550/2210 D Loss: 0.0018 G Loss: 6.5329\n",
      "Epoch [7/100] Batch 600/2210 D Loss: 0.1756 G Loss: 1.5322\n",
      "Epoch [7/100] Batch 650/2210 D Loss: 0.0038 G Loss: 5.6334\n",
      "Epoch [7/100] Batch 700/2210 D Loss: 0.0036 G Loss: 6.2328\n",
      "Epoch [7/100] Batch 750/2210 D Loss: 0.0004 G Loss: 8.1756\n",
      "Epoch [7/100] Batch 800/2210 D Loss: 0.0016 G Loss: 6.5153\n",
      "Epoch [7/100] Batch 850/2210 D Loss: 0.0129 G Loss: 4.6106\n",
      "Epoch [7/100] Batch 900/2210 D Loss: 0.0228 G Loss: 4.1587\n",
      "Epoch [7/100] Batch 950/2210 D Loss: 0.0481 G Loss: 3.5009\n",
      "Epoch [7/100] Batch 1000/2210 D Loss: 0.0231 G Loss: 4.0362\n",
      "Epoch [7/100] Batch 1050/2210 D Loss: 0.0049 G Loss: 5.4575\n",
      "Epoch [7/100] Batch 1100/2210 D Loss: 0.0095 G Loss: 4.7574\n",
      "Epoch [7/100] Batch 1150/2210 D Loss: 0.0959 G Loss: 3.1045\n",
      "Epoch [7/100] Batch 1200/2210 D Loss: 0.0218 G Loss: 3.4049\n",
      "Epoch [7/100] Batch 1250/2210 D Loss: 0.0092 G Loss: 4.9717\n",
      "Epoch [7/100] Batch 1300/2210 D Loss: 0.0378 G Loss: 3.5183\n",
      "Epoch [7/100] Batch 1350/2210 D Loss: 0.0010 G Loss: 6.9207\n",
      "Epoch [7/100] Batch 1400/2210 D Loss: 0.1133 G Loss: 4.3847\n",
      "Epoch [7/100] Batch 1450/2210 D Loss: 0.0135 G Loss: 4.3488\n",
      "Epoch [7/100] Batch 1500/2210 D Loss: 0.0634 G Loss: 3.1122\n",
      "Epoch [7/100] Batch 1550/2210 D Loss: 0.0404 G Loss: 4.3055\n",
      "Epoch [7/100] Batch 1600/2210 D Loss: 0.0345 G Loss: 4.4215\n",
      "Epoch [7/100] Batch 1650/2210 D Loss: 0.1269 G Loss: 3.8253\n",
      "Epoch [7/100] Batch 1700/2210 D Loss: 0.0276 G Loss: 4.0320\n",
      "Epoch [7/100] Batch 1750/2210 D Loss: 0.1151 G Loss: 3.9384\n",
      "Epoch [7/100] Batch 1800/2210 D Loss: 0.0069 G Loss: 5.0895\n",
      "Epoch [7/100] Batch 1850/2210 D Loss: 0.0057 G Loss: 5.0239\n",
      "Epoch [7/100] Batch 1900/2210 D Loss: 0.0033 G Loss: 5.7413\n",
      "Epoch [7/100] Batch 1950/2210 D Loss: 0.0136 G Loss: 4.1568\n",
      "Epoch [7/100] Batch 2000/2210 D Loss: 0.0091 G Loss: 4.9568\n",
      "Epoch [7/100] Batch 2050/2210 D Loss: 0.0003 G Loss: 7.9521\n",
      "Epoch [7/100] Batch 2100/2210 D Loss: 4.0635 G Loss: 4.7560\n",
      "Epoch [7/100] Batch 2150/2210 D Loss: 0.0066 G Loss: 5.0767\n",
      "Epoch [7/100] Batch 2200/2210 D Loss: 0.0061 G Loss: 5.2308\n",
      "Epoch [7/100] D Loss: 0.0017 G Loss: 6.3984\n",
      "Epoch [8/100] Batch 0/2210 D Loss: 0.0025 G Loss: 6.0085\n",
      "Epoch [8/100] Batch 50/2210 D Loss: 0.0037 G Loss: 5.6744\n",
      "Epoch [8/100] Batch 100/2210 D Loss: 0.0022 G Loss: 6.8595\n",
      "Epoch [8/100] Batch 150/2210 D Loss: 0.0017 G Loss: 11.4814\n",
      "Epoch [8/100] Batch 200/2210 D Loss: 0.0535 G Loss: 3.8046\n",
      "Epoch [8/100] Batch 250/2210 D Loss: 0.7919 G Loss: 4.9371\n",
      "Epoch [8/100] Batch 300/2210 D Loss: 0.0080 G Loss: 5.7209\n",
      "Epoch [8/100] Batch 350/2210 D Loss: 0.0061 G Loss: 5.1317\n",
      "Epoch [8/100] Batch 400/2210 D Loss: 0.0138 G Loss: 4.9394\n",
      "Epoch [8/100] Batch 450/2210 D Loss: 0.0458 G Loss: 4.3330\n",
      "Epoch [8/100] Batch 500/2210 D Loss: 0.0015 G Loss: 6.6359\n",
      "Epoch [8/100] Batch 550/2210 D Loss: 0.3858 G Loss: 5.1884\n",
      "Epoch [8/100] Batch 600/2210 D Loss: 0.0194 G Loss: 4.2123\n",
      "Epoch [8/100] Batch 650/2210 D Loss: 0.0613 G Loss: 3.1705\n",
      "Epoch [8/100] Batch 700/2210 D Loss: 0.0359 G Loss: 4.0207\n",
      "Epoch [8/100] Batch 750/2210 D Loss: 0.0209 G Loss: 4.0212\n",
      "Epoch [8/100] Batch 800/2210 D Loss: 0.0396 G Loss: 4.1961\n",
      "Epoch [8/100] Batch 850/2210 D Loss: 0.0117 G Loss: 4.4276\n",
      "Epoch [8/100] Batch 900/2210 D Loss: 0.0013 G Loss: 6.6324\n",
      "Epoch [8/100] Batch 950/2210 D Loss: 0.0019 G Loss: 6.3322\n",
      "Epoch [8/100] Batch 1000/2210 D Loss: 0.0046 G Loss: 5.5339\n",
      "Epoch [8/100] Batch 1050/2210 D Loss: 0.5538 G Loss: 6.0043\n",
      "Epoch [8/100] Batch 1100/2210 D Loss: 0.0736 G Loss: 4.6896\n",
      "Epoch [8/100] Batch 1150/2210 D Loss: 0.0535 G Loss: 3.4387\n",
      "Epoch [8/100] Batch 1200/2210 D Loss: 1.9656 G Loss: 4.7213\n",
      "Epoch [8/100] Batch 1250/2210 D Loss: 0.1998 G Loss: 4.8595\n",
      "Epoch [8/100] Batch 1300/2210 D Loss: 0.0289 G Loss: 4.0482\n",
      "Epoch [8/100] Batch 1350/2210 D Loss: 0.0020 G Loss: 6.3379\n",
      "Epoch [8/100] Batch 1400/2210 D Loss: 0.0815 G Loss: 5.8671\n",
      "Epoch [8/100] Batch 1450/2210 D Loss: 0.0010 G Loss: 7.5278\n",
      "Epoch [8/100] Batch 1500/2210 D Loss: 0.0008 G Loss: 7.1913\n",
      "Epoch [8/100] Batch 1550/2210 D Loss: 0.0013 G Loss: 6.5714\n",
      "Epoch [8/100] Batch 1600/2210 D Loss: 0.2011 G Loss: 5.4344\n",
      "Epoch [8/100] Batch 1650/2210 D Loss: 0.0257 G Loss: 5.0028\n",
      "Epoch [8/100] Batch 1700/2210 D Loss: 0.0035 G Loss: 8.8878\n",
      "Epoch [8/100] Batch 1750/2210 D Loss: 0.0009 G Loss: 7.1033\n",
      "Epoch [8/100] Batch 1800/2210 D Loss: 1.1804 G Loss: 0.4334\n",
      "Epoch [8/100] Batch 1850/2210 D Loss: 1.0044 G Loss: 5.5583\n",
      "Epoch [8/100] Batch 1900/2210 D Loss: 0.0122 G Loss: 6.2869\n",
      "Epoch [8/100] Batch 1950/2210 D Loss: 4.0592 G Loss: 0.9619\n",
      "Epoch [8/100] Batch 2000/2210 D Loss: 0.0106 G Loss: 3.8234\n"
     ]
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\kamen\\\\Dev\\\\School\\\\H25\\\\IFT3710\\\\IFT3710-Advanced-Project-in-ML-AI\\\\data\\\\preprocessing_outputs\\\\unified_set\\\\labels\"\n",
    "z_dim = 256\n",
    "channels = 1\n",
    "model = GANModel(z_dim, channels)\n",
    "data = create_dataloader(path, 1, 256)\n",
    "trainer = Trainer(model, data, device=\"cuda\")\n",
    "trainer.train(epochs=100)\n",
    "evaluator = Evaluator(model, device=\"cuda\")\n",
    "new_samples = evaluator.generate_samples(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
