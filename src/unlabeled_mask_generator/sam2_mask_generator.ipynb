{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa21d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# Lightly adapted from https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c0041e",
   "metadata": {},
   "source": [
    "# Automatically generating object masks with SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289bb0b4",
   "metadata": {},
   "source": [
    "Since SAM 2 can efficiently process prompts, masks for the entire image can be generated by sampling a large number of prompts over an image.\n",
    "\n",
    "The class `SAM2AutomaticMaskGenerator` implements this capability. It works by sampling single-point input prompts in a grid over the image, from each of which SAM can predict multiple masks. Then, masks are filtered for quality and deduplicated using non-maximal suppression. Additional options allow for further improvement of mask quality and quantity, such as running prediction on multiple crops of the image or postprocessing masks to remove small disconnected regions and holes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4290fb06-a63f-4624-a70c-f7c9aae4b5d5",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/sam2/blob/main/notebooks/automatic_mask_generator_example.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b71431",
   "metadata": {},
   "source": [
    "## Environment Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5a78f",
   "metadata": {},
   "source": [
    "If running locally using jupyter, first install `SAM 2` in your environment using the installation instructions in the repository.\n",
    "\n",
    "If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'. Note that it's recommended to use **A100 or L4 GPUs when running in Colab** (T4 GPUs might also work, but could be slow and might run out of memory in some cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941fd2f-a960-4e5f-916b-a5a385bf3a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e9a446-0d12-4b6f-ba93-03fe7453ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install opencv-python matplotlib\n",
    "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam2.git'\n",
    "\n",
    "    !mkdir -p images\n",
    "    !wget -P images https://raw.githubusercontent.com/facebookresearch/sam2/main/notebooks/images/cars.jpg\n",
    "\n",
    "    !mkdir -p ../checkpoints/\n",
    "    !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2bc687",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effad654-436d-400e-97cc-8bd36141370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560725a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ce23c0",
   "metadata": {},
   "source": [
    "## Mask Generation with Sam2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd5ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2MaskUtils import*\n",
    "\n",
    "# --- Initialisation du modèle SAM2 ---\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "\n",
    "sam2_checkpoint = \"../models/sam2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "print(\"Chargement du modèle SAM2...\")\n",
    "sam2 = build_sam2(model_cfg, sam2_checkpoint, device=device, apply_postprocessing=False)\n",
    "mask_generator = SAM2AutomaticMaskGenerator(sam2)\n",
    "print(\"Modèle SAM2 chargé.\")\n",
    "\n",
    "# --- Parcours des images du dossier ---\n",
    "\n",
    "image_dir = \"../../data/unified_set_rename/images/\"\n",
    "label_dir = \"../../data/unified_set_rename/labels/\"\n",
    "output_dir = \"../../data/sam2Labels\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Récupère uniquement les fichiers PNG\n",
    "image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "\n",
    "print(f\"Nombre d'images à traiter : {len(image_files)}\")\n",
    "\n",
    "for image_file in tqdm(image_files, desc=\"Traitement des images\"):\n",
    "    # Extraction du numéro de l'image (exemple: cell_00225.png)\n",
    "    base_name = os.path.splitext(image_file)[0]  # \"cell_00225\"\n",
    "    try:\n",
    "        image_number = base_name.split('_')[1]\n",
    "    except IndexError:\n",
    "        print(f\"Format inattendu pour le nom de fichier : {image_file}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nTraitement de l'image {image_file} (numéro {image_number})\")\n",
    "\n",
    "    # Chargement de l'image et de son annotation\n",
    "    image_path = os.path.join(image_dir, image_file)\n",
    "    label_file = f\"cell_{image_number}_label.tiff\"\n",
    "    label_path = os.path.join(label_dir, label_file)\n",
    "\n",
    "    try:\n",
    "        image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "        imageSoluce = np.array(Image.open(label_path).convert(\"RGB\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement de l'image ou de l'annotation pour {image_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Génération des masques avec SAM2\n",
    "    masks = mask_generator.generate(image)\n",
    "    # Filtrage des masques selon la couleur\n",
    "    filtered_masks = filter_masks_by_color_sam2(\n",
    "        image,\n",
    "        masks,\n",
    "        hue_range=(120, 170),\n",
    "        sat_threshold=30,\n",
    "        min_masks=3\n",
    "    )\n",
    "\n",
    "    # Création des deux versions de masque\n",
    "    all_masks_display = create_grayscale_mask(masks) if len(masks) > 0 else None\n",
    "    filtered_display = create_grayscale_mask_sam2(filtered_masks, image.shape) if len(filtered_masks) > 0 else None\n",
    "\n",
    "    # Calcul des F1 scores\n",
    "    f1_all = calculate_f1(imageSoluce, all_masks_display) if all_masks_display is not None else 0\n",
    "    f1_filtered = calculate_f1(imageSoluce, filtered_display) if filtered_display is not None else 0\n",
    "\n",
    "    print(f\"Image {image_number} : F1 (tous les masques) = {f1_all:.3f} ; F1 (masques filtrés) = {f1_filtered:.3f}\")\n",
    "\n",
    "    # Choix de l'image à enregistrer\n",
    "    if f1_all >= f1_filtered:\n",
    "        chosen_mask = all_masks_display\n",
    "        print(f\"--> Choix de la segmentation avec tous les masques pour l'image {image_number}.\")\n",
    "    else:\n",
    "        chosen_mask = filtered_display\n",
    "        print(f\"--> Choix de la segmentation avec masques filtrés pour l'image {image_number}.\")\n",
    "\n",
    "    # Sauvegarde du masque choisi\n",
    "    if chosen_mask is not None:\n",
    "        # Conversion en format uint8 si nécessaire\n",
    "        if chosen_mask.dtype != np.uint8:\n",
    "            chosen_mask = (chosen_mask * 255).astype(np.uint8)\n",
    "        output_path = os.path.join(output_dir, f\"cell_{image_number}_label_sam2.tiff\")\n",
    "        Image.fromarray(chosen_mask).save(output_path)\n",
    "        print(f\"Résultat sauvegardé : {output_path}\")\n",
    "    else:\n",
    "        print(f\"Aucun masque généré pour l'image {image_number}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
