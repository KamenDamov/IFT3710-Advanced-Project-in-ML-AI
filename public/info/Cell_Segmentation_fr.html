<!DOCTYPE html>
<html>
<head>
<title>Cell_Segmentation_fr.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="segmentation-faiblement-supervis%C3%A9e-des-cellules-dans-des-images-de-microscopie-haute-r%C3%A9solution-et-multi-modalit%C3%A9">Segmentation Faiblement Supervisée des Cellules dans des Images de Microscopie Haute Résolution et Multi-modalité</h1>
<h2 id="description">Description</h2>
<p>Cette tâche provient d’un défi organisé lors de la conférence NeurIPS 2022. Depuis le <a href="https://neurips22-cellseg.grand-challenge.org/">site du défi</a> :</p>
<p>La segmentation des cellules est souvent la première étape des analyses monocellulaires en biologie et recherche biomédicale basées sur les images de microscopie. L’apprentissage profond est largement utilisé pour la segmentation d’images, mais il est difficile de collecter un grand nombre d’images de cellules annotées pour entraîner les modèles, car l’annotation manuelle des cellules est extrêmement chronophage et coûteuse. De plus, les ensembles de données utilisés sont souvent limités à une seule modalité et manquent de diversité, ce qui entraîne une faible généralisation des modèles entraînés. Cette compétition vise à évaluer les méthodes de segmentation cellulaire qui pourraient être appliquées à diverses images de microscopie à travers plusieurs plateformes d’imagerie et types de tissus. Nous formulons le problème de segmentation cellulaire comme une tâche d’apprentissage faiblement supervisée pour encourager les modèles à utiliser des images étiquetées limitées et de nombreuses images non étiquetées, car les images non étiquetées sont relativement faciles à obtenir en pratique.</p>
<p>Cette compétition possède quatre caractéristiques principales :</p>
<ul>
<li>
<p>Cadre de tâche faiblement supervisée : patchs limités étiquetés + nombreuses images non étiquetées ;</p>
</li>
<li>
<p>Vise à évaluer des algorithmes de segmentation cellulaire polyvalents ;</p>
</li>
<li>
<p>Les images de test incluent des images de lame entière (~10,000x10,000) ;</p>
</li>
<li>
<p>Métriques d’évaluation : nous nous concentrons à la fois sur la précision et l’efficacité de la segmentation.</p>
</li>
</ul>
<h2 id="donn%C3%A9es">Données</h2>
<p>La segmentation d’image est une tâche de classification où la classification s’effectue au niveau du pixel. Dans ce cas, le modèle doit apprendre à classer les pixels dans des images de microscopie de manière à segmenter l’image en cellules individuelles. Les images de microscopie peuvent avoir une distribution très complexe, car la structure des images peut varier considérablement selon l’échelle de zoom, le type de cellules observées ou le type de coloration appliqué au milieu. Cet ensemble de données combine des images provenant de diverses plateformes d’imagerie et types de tissus, dans une tentative d’être un échantillon représentatif des types d’imagerie microscopique observés dans le domaine de la biologie.</p>
<p>Cette tâche reflète l’état de nombreux problèmes actuels d’apprentissage automatique, dans la mesure où elle repose sur un mélange de données étiquetées et non étiquetées. L’étiquetage est souvent coûteux, d’où l’intérêt de la recherche pour exploiter les données non étiquetées afin de mieux apprendre la structure de la distribution des données sans les coûts supplémentaires de données étiquetées.</p>
<p>Un autre aspect unique de cet ensemble de données est que certaines images peuvent être extrêmement grandes, allant jusqu’à ~10,000x10,000. Cela constituera un défi computationnel, et vous devrez être créatif dans la conception d’un pipeline qui peut être efficace à ces grandes tailles.</p>
<h2 id="travaux-connexes">Travaux connexes</h2>
<p>Minaee et al. offrent une large revue de nombreuses techniques modernes de segmentation d’images avec une comparaison de performances sur plusieurs ensembles de données [2]. Schmarje et al. passent en revue les méthodes d’apprentissage semi-, auto-, et non supervisées pour la classification d’images [3]. Il sera important de connaître certaines des architectures de vision de base telles que les réseaux convolutifs [4], U-Net [5], et Vision Transformer [6]. Concernant l’imagerie biologique, les méthodes importantes incluent <a href="https://github.com/MouseLand/cellpose">Cellpose</a>, <a href="https://github.com/vanvalenlab/deepcell-tf">Mesmer</a>, <a href="https://github.com/stardist/stardist">Stardist</a>, et <a href="https://github.com/MouseLand/cellpose">Omnipose</a>, toutes ayant des implémentations open source.</p>
<p>Les organisateurs de la compétition ont fourni un <a href="https://github.com/JunMa11/NeurIPS-CellSeg">répertoire GitHub</a> qui démontre l’entraînement de plusieurs architectures différentes (U-Net, ViT+U-Net, et Swin Transformer+U-Net [7]) et évalue le modèle sur des données de validation. [À FAIRE : d’ici à ce que cela soit prêt pour les étudiants, les méthodes et résultats des gagnants peuvent avoir été publiés et devraient être ajoutés ici.]</p>
<h2 id="attentes">Attentes</h2>
<p>Puisque les organisateurs de la compétition ont implémenté des bases raisonnables, il est de votre responsabilité de partir de là ! Votre objectif sera d’implémenter au moins deux nouvelles méthodes qui n’ont pas encore été appliquées à cette tâche d’imagerie microscopique. En réalité, vous devrez essayer de nombreuses combinaisons d’architectures et d’approches pour le problème de taille des images afin de déterminer ce qui fonctionne le mieux. Dans votre rapport final, nous attendons une description des méthodes essayées, avec une comparaison des avantages et inconvénients de chaque méthode tentée.</p>
<h2 id="r%C3%A9f%C3%A9rences">Références</h2>
<ol>
<li>Site du défi : https://neurips22-cellseg.grand-challenge.org/</li>
<li>Minaee et al. (2020) Image segmentation using deep learning: a survey. <a href="https://arxiv.org/abs/2001.05566">arXiv:2001.05566</a></li>
<li>Schmarje et al. (2021) A survey on semi-, self- and unsupervised learning for image classification. <em>IEEE Access</em>. 9: 82146-82168. <a href="https://doi.org/10.1109/ACCESS.2021.3084358">doi:10.1109/ACCESS.2021.3084358</a></li>
<li>LeCun et al. (1989) Backpropagation applied to handwritten zip code recognition. <em>Neural Comput</em>. 1(4): 541-551. <a href="https://doi.org/10.1162/neco.1989.1.4.541">doi:10.1162/neco.1989.1.4.541</a></li>
<li>Ronneberger et al. (2015) U-Net: convolutional networks for biomedical image segmentation. <a href="https://arxiv.org/abs/1505.04597">arXiv:1505.04597</a></li>
<li>Dosovitskiy et al. (2020) An image is worth 16x16 words: transformers for image recognition at scale. <a href="https://arxiv.org/abs/2010.11929v2">arXiv:2010.11929v2</a></li>
<li>Liu et al. (2021) Swin transformer: hierarchical vision transformer using shifted windows. <a href="https://arxiv.org/abs/2103.14030">arXiv:2103.14030</a></li>
<li><a href="https://github.com/MouseLand/cellpose">Cellpose</a></li>
<li><a href="https://github.com/vanvalenlab/deepcell-tf">Mesmer</a></li>
<li><a href="https://github.com/stardist/stardist">Stardist</a></li>
<li><a href="https://github.com/MouseLand/cellpose">Omnipose</a></li>
</ol>

</body>
</html>
