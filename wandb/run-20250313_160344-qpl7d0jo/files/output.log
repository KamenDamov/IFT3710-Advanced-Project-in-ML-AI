C:\Users\kamen\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\torch\optim\lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
learning rate 0.0002000 -> 0.0002000
(epoch: 1, iters: 100, time: 0.229, data: 0.031) G_GAN: 33.354 G_L1: 0.095 D_real: 26.275 D_fake: 33.136
(epoch: 1, iters: 200, time: 0.212, data: 0.008) G_GAN: 22.282 G_L1: 0.102 D_real: 18.904 D_fake: 22.817
(epoch: 1, iters: 300, time: 0.209, data: 0.010) G_GAN: 21.671 G_L1: 0.085 D_real: 17.636 D_fake: 23.294
(epoch: 1, iters: 400, time: 0.204, data: 0.007) G_GAN: 16.220 G_L1: 0.079 D_real: 14.882 D_fake: 16.633
(epoch: 1, iters: 500, time: 0.206, data: 0.009) G_GAN: 5.703 G_L1: 0.079 D_real: 7.258 D_fake: 6.265
(epoch: 1, iters: 600, time: 0.210, data: 0.007) G_GAN: 6.153 G_L1: 0.082 D_real: 5.476 D_fake: 7.888
(epoch: 1, iters: 700, time: 0.187, data: 0.012) G_GAN: 19.025 G_L1: 0.091 D_real: 15.058 D_fake: 19.625
(epoch: 1, iters: 800, time: 0.226, data: 0.011) G_GAN: 5.493 G_L1: 0.078 D_real: 6.819 D_fake: 5.994
(epoch: 1, iters: 900, time: 0.172, data: 0.008) G_GAN: 5.969 G_L1: 0.113 D_real: 10.931 D_fake: 7.126
(epoch: 1, iters: 1000, time: 0.160, data: 0.019) G_GAN: 5.701 G_L1: 0.116 D_real: 7.915 D_fake: 6.297
saving the model at the end of epoch 1, iters 1000
End of epoch 1 / 10 	 Time Taken: 149 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 2, iters: 100, time: 0.209, data: 0.018) G_GAN: 1.022 G_L1: 0.104 D_real: 1.529 D_fake: 1.648
(epoch: 2, iters: 200, time: 0.186, data: 0.013) G_GAN: 2.600 G_L1: 0.081 D_real: 2.744 D_fake: 2.161
(epoch: 2, iters: 300, time: 0.235, data: 0.012) G_GAN: 2.801 G_L1: 0.076 D_real: 3.059 D_fake: 2.179
(epoch: 2, iters: 400, time: 0.245, data: 0.007) G_GAN: 1.735 G_L1: 0.075 D_real: 2.721 D_fake: 1.321
(epoch: 2, iters: 500, time: 0.240, data: 0.009) G_GAN: 1.387 G_L1: 0.073 D_real: 1.256 D_fake: 1.155
(epoch: 2, iters: 600, time: 0.182, data: 0.006) G_GAN: 1.012 G_L1: 0.072 D_real: 0.800 D_fake: 0.984
(epoch: 2, iters: 700, time: 0.184, data: 0.013) G_GAN: 2.507 G_L1: 0.079 D_real: 2.496 D_fake: 2.354
(epoch: 2, iters: 800, time: 0.193, data: 0.008) G_GAN: 1.259 G_L1: 0.069 D_real: 1.447 D_fake: 0.985
(epoch: 2, iters: 900, time: 0.171, data: 0.021) G_GAN: 1.334 G_L1: 0.119 D_real: 2.454 D_fake: 1.662
(epoch: 2, iters: 1000, time: 0.244, data: 0.015) G_GAN: 1.562 G_L1: 0.124 D_real: 1.886 D_fake: 1.328
saving the model at the end of epoch 2, iters 2000
End of epoch 2 / 10 	 Time Taken: 149 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 3, iters: 100, time: 0.199, data: 0.013) G_GAN: 0.412 G_L1: 0.114 D_real: 0.245 D_fake: 0.721
(epoch: 3, iters: 200, time: 0.216, data: 0.017) G_GAN: 1.090 G_L1: 0.077 D_real: 1.299 D_fake: 0.887
(epoch: 3, iters: 300, time: 0.203, data: 0.013) G_GAN: 1.129 G_L1: 0.070 D_real: 1.293 D_fake: 0.572
(epoch: 3, iters: 400, time: 0.198, data: 0.009) G_GAN: 0.808 G_L1: 0.066 D_real: 0.612 D_fake: 0.453
(epoch: 3, iters: 500, time: 0.202, data: 0.014) G_GAN: 0.675 G_L1: 0.052 D_real: 0.530 D_fake: 0.474
(epoch: 3, iters: 600, time: 0.221, data: 0.011) G_GAN: 0.459 G_L1: 0.051 D_real: 0.349 D_fake: 0.559
(epoch: 3, iters: 700, time: 0.242, data: 0.015) G_GAN: 1.187 G_L1: 0.057 D_real: 1.403 D_fake: 0.903
(epoch: 3, iters: 800, time: 0.192, data: 0.007) G_GAN: 0.578 G_L1: 0.044 D_real: 0.562 D_fake: 0.401
(epoch: 3, iters: 900, time: 0.220, data: 0.015) G_GAN: 0.767 G_L1: 0.129 D_real: 1.044 D_fake: 0.603
(epoch: 3, iters: 1000, time: 0.185, data: 0.014) G_GAN: 0.921 G_L1: 0.143 D_real: 0.930 D_fake: 0.487
saving the model at the end of epoch 3, iters 3000
End of epoch 3 / 10 	 Time Taken: 151 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 4, iters: 100, time: 0.234, data: 0.017) G_GAN: 0.331 G_L1: 0.140 D_real: 0.218 D_fake: 0.468
(epoch: 4, iters: 200, time: 0.237, data: 0.007) G_GAN: 0.731 G_L1: 0.052 D_real: 0.959 D_fake: 0.576
(epoch: 4, iters: 300, time: 0.238, data: 0.008) G_GAN: 0.661 G_L1: 0.044 D_real: 0.573 D_fake: 0.411
(epoch: 4, iters: 400, time: 0.240, data: 0.003) G_GAN: 0.635 G_L1: 0.047 D_real: 0.232 D_fake: 0.285
(epoch: 4, iters: 500, time: 0.246, data: 0.007) G_GAN: 0.586 G_L1: 0.040 D_real: 0.363 D_fake: 0.263
(epoch: 4, iters: 600, time: 0.250, data: 0.007) G_GAN: 0.337 G_L1: 0.039 D_real: 0.303 D_fake: 0.338
(epoch: 4, iters: 700, time: 0.229, data: 0.008) G_GAN: 0.664 G_L1: 0.029 D_real: 1.641 D_fake: 0.689
(epoch: 4, iters: 800, time: 0.240, data: 0.007) G_GAN: 0.460 G_L1: 0.035 D_real: 0.393 D_fake: 0.356
(epoch: 4, iters: 900, time: 0.238, data: 0.008) G_GAN: 0.718 G_L1: 0.136 D_real: 0.705 D_fake: 0.260
(epoch: 4, iters: 1000, time: 0.241, data: 0.010) G_GAN: 0.782 G_L1: 0.153 D_real: 0.598 D_fake: 0.276
saving the model at the end of epoch 4, iters 4000
End of epoch 4 / 10 	 Time Taken: 142 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 5, iters: 100, time: 0.244, data: 0.011) G_GAN: 0.299 G_L1: 0.152 D_real: 0.216 D_fake: 0.422
(epoch: 5, iters: 200, time: 0.237, data: 0.007) G_GAN: 0.553 G_L1: 0.042 D_real: 0.663 D_fake: 0.460
(epoch: 5, iters: 300, time: 0.233, data: 0.007) G_GAN: 0.571 G_L1: 0.033 D_real: 0.440 D_fake: 0.368
(epoch: 5, iters: 400, time: 0.241, data: 0.007) G_GAN: 0.559 G_L1: 0.040 D_real: 0.228 D_fake: 0.305
(epoch: 5, iters: 500, time: 0.242, data: 0.007) G_GAN: 0.437 G_L1: 0.034 D_real: 0.323 D_fake: 0.272
(epoch: 5, iters: 600, time: 0.239, data: 0.007) G_GAN: 0.350 G_L1: 0.036 D_real: 0.253 D_fake: 0.219
(epoch: 5, iters: 700, time: 0.240, data: 0.007) G_GAN: 0.834 G_L1: 0.025 D_real: 3.457 D_fake: 0.625
(epoch: 5, iters: 800, time: 0.243, data: 0.007) G_GAN: 0.453 G_L1: 0.039 D_real: 0.406 D_fake: 0.164
(epoch: 5, iters: 900, time: 0.240, data: 0.006) G_GAN: 0.665 G_L1: 0.128 D_real: 0.520 D_fake: 0.175
(epoch: 5, iters: 1000, time: 0.243, data: 0.009) G_GAN: 0.688 G_L1: 0.136 D_real: 0.400 D_fake: 0.196
saving the latest model (epoch 5, total_iters 5000)
