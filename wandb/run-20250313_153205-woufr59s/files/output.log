C:\Users\kamen\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\torch\optim\lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
learning rate 0.0002000 -> 0.0002000
(epoch: 1, iters: 100, time: 0.239, data: 0.072) G_GAN: 27.816 G_L1: 0.051 D_real: 35.061 D_fake: 37.119
(epoch: 1, iters: 200, time: 0.253, data: 0.005) G_GAN: 17.958 G_L1: 0.181 D_real: 22.272 D_fake: 18.450
(epoch: 1, iters: 300, time: 0.257, data: 0.007) G_GAN: 17.413 G_L1: 0.179 D_real: 19.781 D_fake: 17.906
(epoch: 1, iters: 400, time: 0.242, data: 0.007) G_GAN: 13.880 G_L1: 0.176 D_real: 13.438 D_fake: 15.340
(epoch: 1, iters: 500, time: 0.239, data: 0.005) G_GAN: 8.369 G_L1: 0.168 D_real: 7.143 D_fake: 8.508
(epoch: 1, iters: 600, time: 0.239, data: 0.014) G_GAN: 5.501 G_L1: 0.162 D_real: 6.024 D_fake: 5.356
(epoch: 1, iters: 700, time: 0.241, data: 0.010) G_GAN: 9.231 G_L1: 0.200 D_real: 10.452 D_fake: 8.347
(epoch: 1, iters: 800, time: 0.226, data: 0.008) G_GAN: 4.929 G_L1: 0.168 D_real: 9.936 D_fake: 4.905
(epoch: 1, iters: 900, time: 0.229, data: 0.009) G_GAN: 5.100 G_L1: 0.039 D_real: 12.327 D_fake: 4.872
(epoch: 1, iters: 1000, time: 0.183, data: 0.012) G_GAN: 7.002 G_L1: 0.012 D_real: 8.144 D_fake: 7.357
End of epoch 1 / 110 	 Time Taken: 163 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 2, iters: 100, time: 0.200, data: 0.020) G_GAN: 1.314 G_L1: 0.050 D_real: 1.404 D_fake: 1.184
(epoch: 2, iters: 200, time: 0.214, data: 0.012) G_GAN: 2.419 G_L1: 0.181 D_real: 2.512 D_fake: 2.461
(epoch: 2, iters: 300, time: 0.204, data: 0.013) G_GAN: 3.136 G_L1: 0.179 D_real: 3.488 D_fake: 3.012
(epoch: 2, iters: 400, time: 0.250, data: 0.011) G_GAN: 2.693 G_L1: 0.176 D_real: 2.782 D_fake: 1.840
(epoch: 2, iters: 500, time: 0.185, data: 0.008) G_GAN: 1.741 G_L1: 0.167 D_real: 1.433 D_fake: 1.062
(epoch: 2, iters: 600, time: 0.257, data: 0.013) G_GAN: 1.095 G_L1: 0.162 D_real: 1.323 D_fake: 1.237
(epoch: 2, iters: 700, time: 0.233, data: 0.008) G_GAN: 6.778 G_L1: 0.200 D_real: 10.072 D_fake: 5.955
(epoch: 2, iters: 800, time: 0.219, data: 0.009) G_GAN: 1.611 G_L1: 0.168 D_real: 2.625 D_fake: 1.159
(epoch: 2, iters: 900, time: 0.206, data: 0.016) G_GAN: 2.049 G_L1: 0.039 D_real: 4.553 D_fake: 1.883
(epoch: 2, iters: 1000, time: 0.199, data: 0.024) G_GAN: 2.028 G_L1: 0.012 D_real: 2.411 D_fake: 2.290
End of epoch 2 / 110 	 Time Taken: 157 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 3, iters: 100, time: 0.208, data: 0.016) G_GAN: 0.601 G_L1: 0.050 D_real: 0.420 D_fake: 0.563
(epoch: 3, iters: 200, time: 0.214, data: 0.016) G_GAN: 1.066 G_L1: 0.180 D_real: 1.061 D_fake: 1.134
(epoch: 3, iters: 300, time: 0.254, data: 0.016) G_GAN: 1.714 G_L1: 0.178 D_real: 1.941 D_fake: 1.159
(epoch: 3, iters: 400, time: 0.249, data: 0.008) G_GAN: 1.287 G_L1: 0.175 D_real: 1.130 D_fake: 0.614
(epoch: 3, iters: 500, time: 0.249, data: 0.007) G_GAN: 0.915 G_L1: 0.167 D_real: 0.678 D_fake: 0.424
(epoch: 3, iters: 600, time: 0.254, data: 0.007) G_GAN: 0.623 G_L1: 0.162 D_real: 0.564 D_fake: 0.605
(epoch: 3, iters: 700, time: 0.251, data: 0.006) G_GAN: 2.606 G_L1: 0.200 D_real: 3.025 D_fake: 2.321
(epoch: 3, iters: 800, time: 0.255, data: 0.010) G_GAN: 1.052 G_L1: 0.168 D_real: 1.233 D_fake: 0.744
(epoch: 3, iters: 900, time: 0.252, data: 0.008) G_GAN: 1.188 G_L1: 0.039 D_real: 2.206 D_fake: 0.977
(epoch: 3, iters: 1000, time: 0.257, data: 0.009) G_GAN: 1.034 G_L1: 0.012 D_real: 1.154 D_fake: 1.256
End of epoch 3 / 110 	 Time Taken: 148 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 4, iters: 100, time: 0.254, data: 0.009) G_GAN: 0.369 G_L1: 0.050 D_real: 0.432 D_fake: 0.307
(epoch: 4, iters: 200, time: 0.255, data: 0.007) G_GAN: 0.705 G_L1: 0.179 D_real: 0.645 D_fake: 0.714
